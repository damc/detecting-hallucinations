# One token method

"One token" method is a method for detecting hallucinations (specifically, getting the confidence of a model that the answer is correct) in the content generated by large language models.

I haven't shared this idea with too many people so far, and I might be wrong about something, but the empirical evidence generated by this project suggests that it works (see "conclusions" for more information).

In this repository, I empirically test that method by comparing it with simply asking the model to output the confidence.

The project uses TextBridge package https://pypi.org/project/text-bridge/

The project uses [HaluEval](https://github.com/RUCAIBox/HaluEval) dataset to test the method empirically.

# The method

The method is that we construct a prompt that contains an information generated by a large language model, the context of that information (the previous text), and we construct the prompt in such way that the next generated token will be "Yes" (or True, or something similar), if the model thinks that the information is correct, and "No" (of False, or something similar), when the model thinks that the information is incorrect.

Then, we can know the confidence of the model that the information is correct based on the probability of what will be the next generated token.

Here's an example of a prompt that we can use:

```json
schema.json

\`\`\`
{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "type": "array",
  "description": "Dataset for testing detecting hallucinations in the content generated by large language models.",
  "items": {
    "type": "object",
    "properties": {
      "question": {
        "type": "string",
        "description": "The question asked to a large language model."
      },
      "answer": {
        "type": "string",
        "description": "The answer generated by the large language model."
      },
      "is_correct": {
        "type": "boolean",
        "description": "Whether the answer is correct or not."
    },
    "required": ["question", "answer", "is_correct"],
    "additionalProperties": false
  }
}
\`\`\`

dataset.json

\`\`\`json
[
  {
    "question": "What is the capital of France?",
    "answer": "Paris",
    "is_correct": true
  },
  {
    "question": "What is the capital of France?",
    "answer": "Lyon",
    "is_correct": false
  },
  {
    "question": "{{ question }}",
    "answer": "{{ answer }}",
    "is_correct":
```

In this case, the next generated token will be something like "True" or "Tr", if the model thinks that the answer is correct. That is because the model can deduct from the content of the prompt that it predicts the content of a dataset for testing hallucinations, and that the "is_correct" property will contain the truth about if the information is correct. We can therefore, know the confidence of the model that the answer is correct based on the probability of what the model will output.

# Conclusions from testing the method empirically

Conclusions (might depend on the prompt):
1. Simple method of just asking the LLM to output the confidence works a little bit, but not very well.
2. Simple method is too confident, almost always giving probability from 0% to 10% or from 70% to 100%.
3. One token method works well.
4. One token method has a slight bias towards answering that the answer is correct (not a hallucination), except when it answers with confidence from 90% to 100% that the answer is correct (then it's not biased).

Additional info:
1. The bias can be the result of the prompt (with a different prompt, there might be no bias or a different bias).
2. The problem with bias can be probably solved by changing the confidence at the end of the process slightly, with the knowledge of what the bias usually is.
