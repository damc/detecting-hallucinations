# One token method

"One token" method is a method for detecting hallucinations (specifically, getting the confidence of a model that the answer is correct) in the content generated by large language models.

I haven't shared this idea with too many people so far, and I might be wrong about something, but the empirical evidence generated by this project suggests that it works (see "conclusions" for more information).

In this repository, I empirically test that method by comparing it with simply asking the model to output the confidence.

The project uses TextBridge package https://pypi.org/project/text-bridge/

The project uses [HaluEval](https://github.com/RUCAIBox/HaluEval) dataset to test the method empirically.

# The method

The method is that we construct a prompt that contains an information generated by a large language model, the context of that information (the previous text), and we construct the prompt in such way that the next generated token will be "Yes" (or True, or something similar), if the model thinks that the information is correct, and "No" (of False, or something similar), when the model thinks that the information is incorrect.

Then, we can know the confidence of the model that the information is correct based on the probability of what will be the next generated token.

Here's an example of a prompt that we can use:

```json
schema.json

\`\`\`
{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "type": "array",
  "description": "Dataset for testing detecting hallucinations in the content generated by large language models.",
  "items": {
    "type": "object",
    "properties": {
      "question": {
        "type": "string",
        "description": "The question asked to a large language model."
      },
      "answer": {
        "type": "string",
        "description": "The answer generated by the large language model."
      },
      "is_correct": {
        "type": "boolean",
        "description": "Whether the answer is correct or not."
    },
    "required": ["question", "answer", "is_correct"],
    "additionalProperties": false
  }
}
\`\`\`

dataset.json

\`\`\`json
[
  {
    "question": "What is the capital of France?",
    "answer": "Paris",
    "is_correct": true
  },
  {
    "question": "What is the capital of France?",
    "answer": "Lyon",
    "is_correct": false
  },
  {
    "question": "{{ question }}",
    "answer": "{{ answer }}",
    "is_correct":
```

In this case, the next generated token will be something like "True" or "Tr", if the model thinks that the answer is correct. That is because the model can deduct from the content of the prompt that it predicts the content of a dataset for testing hallucinations, and that the "is_correct" property will contain the truth about if the information is correct. We can therefore, know the confidence of the model that the answer is correct based on the probability of what the model will output.

# Conclusions from testing the method empirically

Conclusions (might depend on the prompt and softmax temperature setting):
1. Simple method of just asking the LLM to output the confidence works a little bit, but not very well.
2. Simple method is too confident, almost always giving probability from 0% to 10% or from 70% to 100%.
3. One token method works well.
4. One token method has a slight bias towards answering that the answer is correct (not a hallucination), except when it answers with confidence from 90% to 100% that the answer is correct (then it's not biased).

Additional info:
1. The bias can be the result of the prompt (with a different prompt, there might be no bias or a different bias).
2. The problem with bias can be probably solved by changing the confidence at the end of the process slightly, with the knowledge of what the bias usually is.

# Running the project

Firstly, before you do anything, you need to set OPENAI_API_KEY environment 
variable to your OpenAI API key.

Secondly, you need to install the required packages from requirements.txt. 
If you have `openai` package installed then better do that in the virtual 
environment because it uses TextBridge package which uses an old version of 
OpenAI package which might conflict with your package, if you have a new 
version of `openai` installed.

Then, you can run the project like this:

```commandline
python3 main.py
```

That will ask you for a question and answer and will output the confidence 
that the answer is correct.

But if you want to test the entire batch, then you need to run the tests 
from the `tests` folder. `test_one_token.py` contains the tests for One Token 
method. `tests_simple.py` contains the tests for the simple method of asking 
the model to output the confidence (for comparison). You can run it with 
PyTest for example.

There are two methods of testing it - error and bucket. Check the docstrings 
in the file, if you want to know what those methods test exactly.